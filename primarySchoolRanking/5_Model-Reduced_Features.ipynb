{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain:\n",
    "#### Ofsted rating is a manual process dependent on an ofsted inspector physically visiting the school. \n",
    "#### Schools are rated in four categories, 1.Outstanding 2. Good 3. Satisfactory 4. Inadequate\n",
    "#### Osted can perform a limited number of visits per year(appr 1200 visits every year among  primary16000 schools), so they usually inspect \n",
    "- a 'satisfactory' school every 5 years(EDA shows much more) or so.\n",
    "- a 'good' school every 3 years((EDA shows much more) or so.\n",
    "- an 'oustanding' school, only when they receive a complaint\n",
    "\n",
    "## Problem : \n",
    "#### A school can have an incorrect ofsted rating for many years, until a new inspection is carried out in who knows 5/10/15 years!!!\n",
    "#### Ideal solution is to have school inspections every year for each school, but this will be very costly as we need physical school inspection.\n",
    "\n",
    "## Solution : \n",
    "#### Predict ofsted ratings of schools, so that we identify schools, that need ofsted manual inspection the most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does this notebook do?\n",
    "- load data for primary schools that were ofsted inspected in year 2016-2017\n",
    "- perform a test-train split on 2016-2017 data.\n",
    "- baseline model score to predict ofsted ratings\n",
    "- perform gridsearch on 3 different models with various scorers\n",
    "- display/plot the scores/metrics of all models for the test data from 2016-2017.\n",
    "- identify best model, and draw conculsions.\n",
    "- Bonus: \n",
    " - test how these models perform on year 2015-2016 and 2017-2018 data.\n",
    " - display/plot the scores/metrics of all models for 2015-2016 and 2017-2018 data.\n",
    " - Testing the solution : identify schools that need ofsted manual inspection the most, for for 2015-2016 and 2017-2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion : \n",
    "- To classify a school as outstanding(1) or inadequate(4) with reduced feature set\n",
    " - we can see \"LogisticRegression'>_minority_class_recall_score\" model works the best with i.e. {'C': 0.001, 'penalty': 'l2', 'solver': 'liblinear'}\n",
    " - it has a minority_class_recall_score of 82.55%. \n",
    "- With this model, we can identify the schools that need the ofsted physical inspection the most\n",
    " - i.e. the schools whose current and predicted ofsted differs a lot\n",
    " - e.g. a school with current_ofsted=1 and predicted_ofsted=4 needs ofsted physical inspection the most\n",
    "\n",
    "#### Note :\n",
    "- minority_class_recall_score = we have identified x% of the school that fall in category 1 or 4. i.e. \n",
    "- we have used feature redcution here, using regularization run in an earlier notebook. We noticed overall recall went up but minority_class_recall_score went down. so this is not the best model/feature\n",
    "- baseline model had minority_class_recall_score: 34.13%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START loading util functions\n",
      "DONE loading util functions\n"
     ]
    }
   ],
   "source": [
    "%run import_util.py\n",
    "import scan_api\n",
    "import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data for schools that were ofsted inspected in year 2016-2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameters that you might want to reset in the notebook\n",
    "\n",
    "# util.np.random.seed(42)\n",
    "# util.trace=False\n",
    "util.debug=True\n",
    "# util.info=True\n",
    "# util.multiclass=True\n",
    "# util.upsample=True\n",
    "\n",
    "# util.myscorers = [\n",
    "#     util.multiclass_accuracy_score,\n",
    "#     util.multiclass_recall_score,\n",
    "#     util.multiclass_f1_score,\n",
    "#     util.minority_class_recall_score,\n",
    "#     util.poor_class_recall_score\n",
    "# ]\n",
    "n=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on EDA, we found 600+ school data missing total_income, total_expenditure,free_school_meal_band . So we should drop them from the feature set\n",
    "# based on regularization, we found only these features useful\n",
    "\n",
    "# set features and target\n",
    "features=['readprog', 'ptread_exp','ptread_high', 'read_average', \n",
    "          'writeprog', 'ptgps_exp', 'ptgps_high','gps_average', \n",
    "          'matprog', 'ptmat_exp', 'ptmat_high', 'ptmat_average']\n",
    "\n",
    "target=\"ofsted\"\n",
    "\n",
    "all=['URN']+features+['ofsted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URN</th>\n",
       "      <th>abscence</th>\n",
       "      <th>persitent_abscence</th>\n",
       "      <th>total_pupils</th>\n",
       "      <th>girls_perc</th>\n",
       "      <th>english_nfl</th>\n",
       "      <th>free_school_meal</th>\n",
       "      <th>is_london</th>\n",
       "      <th>total_income</th>\n",
       "      <th>total_expenditure</th>\n",
       "      <th>...</th>\n",
       "      <th>ptgps_exp</th>\n",
       "      <th>ptgps_high</th>\n",
       "      <th>gps_average</th>\n",
       "      <th>matprog</th>\n",
       "      <th>ptmat_exp</th>\n",
       "      <th>ptmat_high</th>\n",
       "      <th>ptmat_average</th>\n",
       "      <th>ofsted</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>ofsted_phase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.8</td>\n",
       "      <td>276.0</td>\n",
       "      <td>49.3</td>\n",
       "      <td>59.3</td>\n",
       "      <td>12.0</td>\n",
       "      <td>True</td>\n",
       "      <td>8176.0</td>\n",
       "      <td>8319.0</td>\n",
       "      <td>...</td>\n",
       "      <td>92.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>92.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-04-19</td>\n",
       "      <td>Primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>136807</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.5</td>\n",
       "      <td>92.0</td>\n",
       "      <td>44.6</td>\n",
       "      <td>59.7</td>\n",
       "      <td>13.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2013-07-04</td>\n",
       "      <td>Primary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      URN  abscence  persitent_abscence  total_pupils  girls_perc  \\\n",
       "0  100000       2.5                 2.8         276.0        49.3   \n",
       "1  136807       3.8                 6.5          92.0        44.6   \n",
       "\n",
       "   english_nfl  free_school_meal  is_london  total_income  total_expenditure  \\\n",
       "0         59.3              12.0       True        8176.0             8319.0   \n",
       "1         59.7              13.0      False           NaN                NaN   \n",
       "\n",
       "   ...  ptgps_exp  ptgps_high  gps_average  matprog  ptmat_exp  ptmat_high  \\\n",
       "0  ...       92.0        50.0        109.0      3.1       92.0        38.0   \n",
       "1  ...        NaN         NaN          NaN      NaN        NaN         NaN   \n",
       "\n",
       "   ptmat_average  ofsted    pub_date ofsted_phase  \n",
       "0          107.0     1.0  2013-04-19      Primary  \n",
       "1            NaN     2.0  2013-07-04      Primary  \n",
       "\n",
       "[2 rows x 37 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have three years of school data\n",
    "start=['2016','2015','2017']\n",
    "end=['2017','2016','2018']\n",
    "\n",
    "# we are loading 2016_2017 data, and mdoelling on it\n",
    "util.data_directory=\"./data/\"+start[0]+\"-\"+end[0]+\"/\"\n",
    "df=util.read_file(\"cleanData1.csv\")\n",
    "df.head(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16785, 37)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2.0    11576\n",
       "1.0     3063\n",
       "3.0     1333\n",
       "4.0      214\n",
       "Name: ofsted, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2006, 37)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2.0    943\n",
       "3.0    736\n",
       "4.0    166\n",
       "1.0    161\n",
       "Name: ofsted, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.shape)\n",
    "display(df['ofsted'].value_counts())\n",
    "\n",
    "# filter out schools which were ofsted inspected this year\n",
    "df['pub_date'] = df.pub_date.apply(util.convert_to_datetime) \n",
    "df.dropna(axis=0, how='any', subset=['pub_date'],inplace=True)\n",
    "start_date = datetime.strptime(start[0]+'-08-01', '%Y-%m-%d')\n",
    "end_date = datetime.strptime(end[0]+'-07-30', '%Y-%m-%d')\n",
    "df = df[ (df['pub_date']>start_date) & (df['pub_date']<end_date) ]\n",
    "\n",
    "# convert boolean feature to float\n",
    "df['is_london'].replace(True, 1,inplace=True)\n",
    "df['is_london'].replace(False, 0,inplace=True)\n",
    "\n",
    "display(df.shape)\n",
    "display(df['ofsted'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2006, 14)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2.0    943\n",
       "3.0    736\n",
       "4.0    166\n",
       "1.0    161\n",
       "Name: ofsted, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1741, 14)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2.0    803\n",
       "3.0    646\n",
       "4.0    150\n",
       "1.0    142\n",
       "Name: ofsted, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "URN              0\n",
       "readprog         0\n",
       "ptread_exp       0\n",
       "ptread_high      0\n",
       "read_average     0\n",
       "writeprog        0\n",
       "ptgps_exp        0\n",
       "ptgps_high       0\n",
       "gps_average      0\n",
       "matprog          0\n",
       "ptmat_exp        0\n",
       "ptmat_high       0\n",
       "ptmat_average    0\n",
       "ofsted           0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URN</th>\n",
       "      <th>readprog</th>\n",
       "      <th>ptread_exp</th>\n",
       "      <th>ptread_high</th>\n",
       "      <th>read_average</th>\n",
       "      <th>writeprog</th>\n",
       "      <th>ptgps_exp</th>\n",
       "      <th>ptgps_high</th>\n",
       "      <th>gps_average</th>\n",
       "      <th>matprog</th>\n",
       "      <th>ptmat_exp</th>\n",
       "      <th>ptmat_high</th>\n",
       "      <th>ptmat_average</th>\n",
       "      <th>ofsted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>100044</td>\n",
       "      <td>1.6</td>\n",
       "      <td>69.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>96.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>96.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>100168</td>\n",
       "      <td>5.4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>97.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>91.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       URN  readprog  ptread_exp  ptread_high  read_average  writeprog  \\\n",
       "38  100044       1.6        69.0         12.0         104.0        1.8   \n",
       "83  100168       5.4       100.0         56.0         111.0        1.5   \n",
       "\n",
       "    ptgps_exp  ptgps_high  gps_average  matprog  ptmat_exp  ptmat_high  \\\n",
       "38       96.0        62.0        110.0      4.8       96.0        27.0   \n",
       "83       97.0        63.0        111.0      2.9       91.0        38.0   \n",
       "\n",
       "    ptmat_average  ofsted  \n",
       "38          107.0     2.0  \n",
       "83          108.0     1.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  drop all rows which have NAN \n",
    "\n",
    "df = df[all]\n",
    "\n",
    "display(df.shape)\n",
    "display(df['ofsted'].value_counts())\n",
    "df.dropna(axis=0, how='any',inplace=True)\n",
    "display(df.shape)\n",
    "display(df['ofsted'].value_counts())\n",
    "\n",
    "display(df.isnull().sum())\n",
    "df.head(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the modelling is to be done on binary label, convert multiclass to binary\n",
    "if not util.multiclass:\n",
    "    display(df.shape)\n",
    "    display(df['ofsted'].value_counts())\n",
    "    \n",
    "    df['ofsted'].replace([1, 2], 1,inplace=True)\n",
    "    df['ofsted'].replace([3, 4], 0,inplace=True)\n",
    "\n",
    "    display(df.shape)\n",
    "    display(df['ofsted'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a test-train split on 2016-2017 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    602\n",
       "3.0    484\n",
       "4.0    112\n",
       "1.0    107\n",
       "Name: ofsted, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = df[features]\n",
    "y = df[target].values\n",
    "\n",
    "#scale features, by default target is not scaled\n",
    "#note:stratify=y\n",
    "scalerX, scalery, X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = util.split_scale_df(X,y,stratify=y)\n",
    "\n",
    "display(pd.DataFrame(y_train_scaled,columns=['ofsted'])['ofsted'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0    602\n",
      "3.0    484\n",
      "4.0    112\n",
      "1.0    107\n",
      "Name: ofsted, dtype: int64\n",
      "2.0    602\n",
      "4.0    560\n",
      "1.0    535\n",
      "3.0    484\n",
      "Name: ofsted, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# upsample_minority_class \n",
    "if util.upsample:\n",
    "    new_df=X_train_scaled.copy()\n",
    "    new_df[target]=y_train_scaled\n",
    "    if util.multiclass:\n",
    "        new_df = util.upsample_minority_class_multiclass(new_df)\n",
    "    else:\n",
    "        new_df = util.upsample_minority_class_binary(new_df)\n",
    "    X_train_scaled = new_df[features]\n",
    "    y_train_scaled = new_df[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URN</th>\n",
       "      <th>readprog</th>\n",
       "      <th>ptread_exp</th>\n",
       "      <th>ptread_high</th>\n",
       "      <th>read_average</th>\n",
       "      <th>writeprog</th>\n",
       "      <th>ptgps_exp</th>\n",
       "      <th>ptgps_high</th>\n",
       "      <th>gps_average</th>\n",
       "      <th>matprog</th>\n",
       "      <th>ptmat_exp</th>\n",
       "      <th>ptmat_high</th>\n",
       "      <th>ptmat_average</th>\n",
       "      <th>ofsted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>100044</td>\n",
       "      <td>1.6</td>\n",
       "      <td>69.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>96.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>96.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>100168</td>\n",
       "      <td>5.4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>97.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>91.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       URN  readprog  ptread_exp  ptread_high  read_average  writeprog  \\\n",
       "38  100044       1.6        69.0         12.0         104.0        1.8   \n",
       "83  100168       5.4       100.0         56.0         111.0        1.5   \n",
       "\n",
       "    ptgps_exp  ptgps_high  gps_average  matprog  ptmat_exp  ptmat_high  \\\n",
       "38       96.0        62.0        110.0      4.8       96.0        27.0   \n",
       "83       97.0        63.0        111.0      2.9       91.0        38.0   \n",
       "\n",
       "    ptmat_average  ofsted  \n",
       "38          107.0     2.0  \n",
       "83          108.0     1.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model score to predict ofsted ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline metrics using DummyClassifier\n",
      "most_frequent based score : 0.4610091743119266\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:blue'>test poor_class_recall_score: 0.0, test minority_class_recall_score: 0.0, test multiclass_f1_score: 0.631083202511774, test multiclass_recall_score: 0.25, test multiclass_accuracy_score: 0.4610091743119266, </span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stratified based score : 0.24541284403669725\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:blue'>test poor_class_recall_score: 0.2894736842105263, test minority_class_recall_score: 0.2733082706766917, test multiclass_f1_score: 0.22314855662728542, test multiclass_recall_score: 0.2599727275624107, test multiclass_accuracy_score: 0.25229357798165136, </span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uniform based score : 0.24770642201834864\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:blue'>test poor_class_recall_score: 0.3684210526315789, test minority_class_recall_score: 0.34135338345864663, test multiclass_f1_score: 0.2433931944602081, test multiclass_recall_score: 0.29853278165008984, test multiclass_accuracy_score: 0.26605504587155965, </span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior based score : 0.4610091743119266\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:blue'>test poor_class_recall_score: 0.0, test minority_class_recall_score: 0.0, test multiclass_f1_score: 0.631083202511774, test multiclass_recall_score: 0.25, test multiclass_accuracy_score: 0.4610091743119266, </span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "print(\"baseline metrics using DummyClassifier\")\n",
    "for new_strategy in [\"most_frequent\",\"stratified\",\"uniform\",\"prior\"]:\n",
    "    dummy_classifier = DummyClassifier(strategy=new_strategy)\n",
    "    dummy_classifier.fit( X_train_scaled,y_train_scaled )\n",
    "    print(f\"{new_strategy} based score : {dummy_classifier.score(X_test_scaled, y_test_scaled)}\")\n",
    "    y_test_pred=dummy_classifier.predict(X_test_scaled)\n",
    "    to_print=\"\"\n",
    "    for my_scorer in util.myscorers:\n",
    "        scorer_str=(str(my_scorer).split(\" \")[1])\n",
    "        to_print=f\"test {scorer_str}: {my_scorer(y_test_scaled, y_test_pred)}\"+ \", \" + to_print\n",
    "    util.printmd(to_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform gridsearch on 3 different models with various scorers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_gridsearchcv(X_train, X_test, y_train, y_test, estimator, scorer):\n",
    "    reg=True\n",
    "    if str(type(estimator))==\"<class 'sklearn.neighbors.classification.KNeighborsClassifier'>\":\n",
    "        param_grid={'n_neighbors': range(3,20,2)}\n",
    "        reg=False\n",
    "    elif str(type(estimator))==\"<class 'sklearn.ensemble.forest.RandomForestClassifier'>\":\n",
    "        param_grid={\"n_estimators\": [9, 11, 13, 15, 17, 19, 21],\n",
    "                                \"min_samples_leaf\": [5, 10, 25],\n",
    "                                \"max_depth\": [3, 7, 10]}\n",
    "        reg=False\n",
    "    else:\n",
    "        param_grid=[{'penalty':['l1'],'C': np.logspace(-3, 3, 10),'solver':['liblinear']},\n",
    "                    {'penalty':['l2'],'C': np.logspace(-3, 3, 10),'solver':['liblinear']}]\n",
    "    \n",
    "    print(\"start\")   \n",
    "    grid = df_gridsearchcv(X_train, y_train, estimator,param_grid,scorer,4,refit=scorer)\n",
    "    \n",
    "    \n",
    "    y_train_pred = grid.best_estimator_.predict(X_train)\n",
    "    print(\"end\")\n",
    "    y_test_pred = grid.best_estimator_.predict(X_test)\n",
    "    \n",
    "    to_print=\"\"\n",
    "    if debug:\n",
    "        for my_scorer in myscorers:\n",
    "            a=(str(my_scorer).split(\" \")[1])\n",
    "            to_print=f\"train {a}: {my_scorer(y_train, y_train_pred)}\"+ \", \" + to_print\n",
    "        print(to_print)\n",
    "        if reg:\n",
    "            print(f\"intercept: {grid.best_estimator_.intercept_}, coef{X_train.columns} : {grid.best_estimator_.coef_}\")\n",
    "    \n",
    "    to_print=\"\"\n",
    "    for my_scorer in myscorers:\n",
    "        a=(str(my_scorer).split(\" \")[1])\n",
    "        to_print=f\"test {a}: {my_scorer(y_test, y_test_pred)}\"+ \", \" + to_print\n",
    "    printmd(to_print)\n",
    "    return grid, y_test,y_test_pred\n",
    "\n",
    "def df_gridsearchcv(X_train, y_train, estimator, param_grid,scoring,cv,refit):\n",
    "    grid = GridSearchCV(estimator=estimator,\n",
    "                    param_grid=param_grid,\n",
    "                    scoring=scoring,\n",
    "                    return_train_score=True,\n",
    "                    cv=cv, \n",
    "                    iid=True,\n",
    "                    refit=refit,\n",
    "                    error_score=np.nan)\n",
    "\n",
    "    grid.fit(X=X_train,y=y_train)\n",
    "    if debug:\n",
    "        print(f\"best_estimator: {grid.best_estimator_}\")\n",
    "        print(f\"best estimator score: {grid.best_score_},best params: {grid.best_params_}\")  \n",
    "    if trace:\n",
    "        display(pd.DataFrame(grid.cv_results_)[['params','mean_test_score']])\n",
    "    return grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier'>_multiclass_accuracy_score\n",
      "start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\priya\\dev\\applications\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-13-089de22e9561>\", line 20, in <module>\n",
      "    estimator,scorer=util.make_scorer(my_scorer))\n",
      "  File \"<ipython-input-12-40284770d253>\", line 16, in classifier_gridsearchcv\n",
      "    grid = df_gridsearchcv(X_train, y_train, estimator,param_grid,scorer,4,refit=scorer)\n",
      "  File \"<ipython-input-12-40284770d253>\", line 49, in df_gridsearchcv\n",
      "    grid.fit(X=X_train,y=y_train)\n",
      "  File \"C:\\Users\\priya\\dev\\applications\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\", line 697, in fit\n",
      "    self.best_index_ = self.refit(results)\n",
      "TypeError: __call__() missing 2 required positional arguments: 'X' and 'y_true'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\priya\\dev\\applications\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\priya\\dev\\applications\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\priya\\dev\\applications\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\priya\\dev\\applications\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\priya\\dev\\applications\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\priya\\dev\\applications\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\priya\\dev\\applications\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\priya\\dev\\applications\\Anaconda3\\lib\\inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "AttributeError: module has no attribute '__name__'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__call__() missing 2 required positional arguments: 'X' and 'y_true'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Run knn, log_reg and random_forest on various scorers\n",
    "i=0\n",
    "df_plot= pd.DataFrame(columns=['model_name','scorer','scores'])\n",
    "grids = {}\n",
    "# trace=True\n",
    "# debug=True\n",
    "for estimator in [KNeighborsClassifier(),\n",
    "                   LogisticRegression(multi_class='ovr',max_iter=100),\n",
    "                   RandomForestClassifier()]:\n",
    "    for my_scorer in util.myscorers:\n",
    "        scorer_str=str(my_scorer).split(\" \")[1]\n",
    "        estimator_str=str(type(estimator)).split(\".\")[3]\n",
    "        run_name=estimator_str+\"_\"+scorer_str\n",
    "        print(run_name)\n",
    "#         grid, y_test, y_test_pred = util.classifier_gridsearchcv(X_train_scaled, \n",
    "        grid, y_test, y_test_pred = classifier_gridsearchcv(X_train_scaled, \n",
    "                                                      X_test_scaled,\n",
    "                                                      y_train_scaled,\n",
    "                                                      y_test_scaled,\n",
    "                                                      estimator,scorer=util.make_scorer(my_scorer))\n",
    "        for my_scorer1 in util.myscorers:\n",
    "            scorer_str1=str(my_scorer1).split(\" \")[1]\n",
    "            df_plot.loc[i] = [estimator_str+\":\"+scorer_str,\n",
    "                              scorer_str1,\n",
    "                              my_scorer1(y_test,y_test_pred)*100]\n",
    "            i=i+1\n",
    "        \n",
    "        grids[run_name]= (grid, y_test, y_test_pred)\n",
    "\n",
    "# trace=False\n",
    "# debug=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display/plot the scores/metrics of all models for the test data from 2016-2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display test data scores of various best models  \n",
    "df1 = df_plot.pivot(index='model_name', columns='scorer', values='scores')\n",
    "df1.plot(kind='bar',figsize=(20, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display test confusion matrix of various best models  \n",
    "for gridName,gridValues in grids.items():\n",
    "    print(gridName)\n",
    "    grid, y_test, y_test_pred = gridValues\n",
    "    display(pd.crosstab(y_test, y_test_pred, rownames=['Actual'], colnames=['Predicted'], margins=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display test data stats of various best models  \n",
    "\n",
    "for gridName,gridValues in grids.items():\n",
    "    print(gridName)\n",
    "    grid, y_test, y_test_pred = gridValues\n",
    "    print(classification_report(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify best model, and draw conculsions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conclusion: \n",
    "# To classify a school as outstanding(1) or inadequate(4)\n",
    "# we can see \"LogisticRegression'>_minority_class_recall_score\" model works the best with an recall of 82.55%, {'C': 0.001, 'penalty': 'l2', 'solver': 'liblinear'}\n",
    "\n",
    "# With this model, we can identify the schools that need the ofsted physical inspection the most\n",
    "# i.e. the schools whose current and predicted ofsted differs a lot\n",
    "# e.g. a school with current_ofsted=1 and predicted_ofsted=4 needs ofsted physical inspection the most\n",
    "\n",
    "# Notes:\n",
    "# recal_minority_class_recall_score = we have identified x% of the school that fall in category 1 or 4. i.e. \n",
    "# we have used feature redcution here, using regularization run in an earlier notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus : Test how these models perform on year 2015-2016 and 2017-2018 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate test data scores for other years by running our best models on them\n",
    "\n",
    "df_plots={}\n",
    "df_school_visit_plots={}\n",
    "for i in [1,2]:\n",
    "    util.data_directory=\"./data/\"+start[i]+\"-\"+end[i]+\"/\" \n",
    "    util.printmd(f\"stats for {util.data_directory}\")\n",
    "    df1 = pd.read_csv(util.data_directory + \"cleanData1.csv\")\n",
    "\n",
    "    df1['pub_date'] = df1.pub_date.apply(util.convert_to_datetime) \n",
    "    df1.dropna(axis=0, how='any', subset=['pub_date'],inplace=True)\n",
    "    start_date = datetime.strptime(start[i]+'-08-01', '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(end[i]+'-07-30', '%Y-%m-%d')\n",
    "    df1 = df1[ (df1['pub_date']>start_date) & (df1['pub_date']<end_date) ]\n",
    "    \n",
    "    if not util.multiclass:\n",
    "        df1['ofsted'].replace([1, 2], 1,inplace=True)\n",
    "        df1['ofsted'].replace([3, 4], 0,inplace=True)\n",
    "            \n",
    "    df1 = df1[all]\n",
    "    df1.dropna(axis=0, how='any',inplace=True)\n",
    "    display(df1['ofsted'].value_counts())\n",
    "    display(df1.shape)\n",
    "\n",
    "    X1 = df1[features]\n",
    "    y1 = df1[target].values\n",
    "\n",
    "    X1_test_scaled = pd.DataFrame(scalerX.transform(X1), index=X1.index, columns=X1.columns)\n",
    "    y1_test_scaled = y1\n",
    "\n",
    "    predicted_school_df={}\n",
    "    count=0\n",
    "    df_plot1= pd.DataFrame(columns=['model_name','scorer','scores'])\n",
    "    for gridName,gridValues in grids.items():\n",
    "        print(f\"stats for {gridName}\")\n",
    "        grid, y_test, y_test_pred = gridValues\n",
    "        y1_test_pred = grid.best_estimator_.predict(X1_test_scaled)\n",
    "        new_df=df1[['URN',target]].copy()\n",
    "        new_df['ofsted_predicted']=y1_test_pred\n",
    "        predicted_school_df[gridName]=new_df\n",
    "        print(f\"best_estimator_.score: {grid.best_estimator_.score(X1_test_scaled, y1_test_scaled)}\")\n",
    "        \n",
    "        to_print=\"\"\n",
    "        for my_scorer in util.myscorers:\n",
    "            scorer_str=(str(my_scorer).split(\" \")[1])\n",
    "            to_print=f\"test {scorer_str}: {my_scorer(y1_test_scaled, y1_test_pred)}\"+ \", \" + to_print\n",
    "            df_plot1.loc[count] = [gridName,\n",
    "                          scorer_str,\n",
    "                          my_scorer(y1_test_scaled,y1_test_pred)*100]\n",
    "            count=count+1\n",
    "        \n",
    "        util.printmd(to_print)   \n",
    "        display(pd.crosstab(y1_test_scaled, y1_test_pred, rownames=['Actual'], colnames=['Predicted'], margins=True))\n",
    "        print(classification_report(y1_test_scaled,y1_test_pred))\n",
    "    df_plots[start[i]+\"-\"+end[i]]=df_plot1\n",
    "    df_school_visit_plots[start[i]+\"-\"+end[i]]=predicted_school_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Display/plot the scores/metrics of all models for 2015-2016 and 2017-2018 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display test data scores for other years by running our best models on them\n",
    "for df_plot1 in df_plots.values():\n",
    "    df1 = df_plot1.pivot(index='model_name', columns='scorer', values='scores')\n",
    "    df1.plot(kind='bar',figsize=(20, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the solution : Identify schools that need ofsted manual inspection the most, for 2015-2016 and 2017-2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year,predicted_school_df in df_school_visit_plots.items():\n",
    "    no_of_schools_to_visit_values=[]\n",
    "    ofsted_gap_threshold=3\n",
    "    for model_name,df in predicted_school_df.items():\n",
    "        df['ofsted_gap']=abs(df['ofsted_predicted']-df['ofsted'])\n",
    "        no_of_schools_to_visit=(df[df['ofsted_gap']>=ofsted_gap_threshold].shape)[0]\n",
    "        no_of_schools_to_visit_values=no_of_schools_to_visit_values+[no_of_schools_to_visit]\n",
    "\n",
    "    df_visit_plot= pd.DataFrame(columns=['model_name','schools_to_visit'])\n",
    "    df_visit_plot['model_name']=predicted_school_df.keys()\n",
    "    df_visit_plot['schools_to_visit']=no_of_schools_to_visit_values\n",
    "\n",
    "    print(f\"schools to visit {year}\")\n",
    "    df_visit_plot.set_index('model_name',inplace=True)\n",
    "    df_visit_plot.plot(kind='bar',figsize=(10, 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
